{
 "metadata": {
  "name": "",
  "signature": "sha256:4945b2a6f7d790c4e4ad62079bba057d514f04bf5275a3e57461ee1ce814fc0b"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# code to generate sample multiclass 1-vs-all data sets\n",
      "# data set is very small and needs to be replaced with rcv\n",
      "#\n",
      "#  this example will use the 20 newsgroup set\n",
      "#\n",
      "# see: http://scikit-learn.org/stable/auto_examples/document_classification_20newsgroups.html#example-document-classification-20newsgroups-py\n",
      "# \n",
      "import sys\n",
      "from time import time\n",
      "from pprint import pprint\n",
      "\n",
      "import numpy as np\n",
      "import scipy\n",
      "import scipy.sparse as sp\n",
      "import joblib\n",
      "\n",
      "import io\n",
      "import os.path\n",
      "\n",
      "import sklearn\n",
      "import sklearn.svm\n",
      "import sklearn.datasets\n",
      "import sklearn.metrics\n",
      "import sklearn.cross_validation\n",
      "\n",
      "\n",
      "from sklearn.externals.six import u, b\n",
      "\n",
      "from sklearn.datasets import fetch_20newsgroups\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "from sklearn.feature_extraction.text import HashingVectorizer\n",
      "from sklearn.svm import LinearSVC\n",
      "from sklearn.feature_selection import SelectKBest, chi2\n",
      "\n",
      "import warnings\n",
      "warnings.filterwarnings('ignore')\n",
      "\n",
      "%pylab inline"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Populating the interactive namespace from numpy and matplotlib\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "categories = None\n",
      "remove = ('headers', 'footers', 'quotes')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(\"Loading 20 newsgroups dataset for categories:\")\n",
      "\n",
      "data_train = fetch_20newsgroups(subset='train', categories=categories,\n",
      "                                shuffle=True, random_state=42,\n",
      "                                remove=remove)\n",
      "\n",
      "data_test = fetch_20newsgroups(subset='test', categories=categories,\n",
      "                               shuffle=True, random_state=42,\n",
      "                               remove=remove)\n",
      "print('data loaded')\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Loading 20 newsgroups dataset for categories:\n",
        "data loaded"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "categories = data_train.target_names    # for case categories == None\n",
      "\n",
      "\n",
      "def size_mb(docs):\n",
      "    return sum(len(s.encode('utf-8')) for s in docs) / 1e6\n",
      "\n",
      "data_train_size_mb = size_mb(data_train.data)\n",
      "data_test_size_mb = size_mb(data_test.data)\n",
      "\n",
      "print(\"%d documents - %0.3fMB (training set)\" % (\n",
      "    len(data_train.data), data_train_size_mb))\n",
      "print(\"%d documents - %0.3fMB (test set)\" % (\n",
      "    len(data_test.data), data_test_size_mb))\n",
      "print(\"%d categories\" % len(categories))\n",
      "print()\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "11314 documents - 13.782MB (training set)\n",
        "7532 documents - 8.262MB (test set)\n",
        "20 categories\n",
        "()\n"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# split a training set and a test set\n",
      "# we will eventually split the training up into L and U, and use test as the HO\n",
      "y_train, y_test = data_train.target, data_test.target"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(\"Extracting features from the training dataset using a sparse vectorizer\")\n",
      "vectorizer = TfidfVectorizer(sublinear_tf=True, max_df=0.5,\n",
      "                                 stop_words='english')\n",
      "X_train = vectorizer.fit_transform(data_train.data)\n",
      "print(\"n_samples: %d, n_features: %d\" % X_train.shape)\n",
      "print()\n",
      "\n",
      "print(\"Extracting features from the test dataset using the same vectorizer\")\n",
      "t0 = time()\n",
      "X_test = vectorizer.transform(data_test.data)\n",
      "print(\"n_samples: %d, n_features: %d\" % X_test.shape)\n",
      "print()\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Extracting features from the training dataset using a sparse vectorizer\n",
        "n_samples: 11314, n_features: 101323"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "()\n",
        "Extracting features from the test dataset using the same vectorizer\n",
        "n_samples: 7532, n_features: 101323"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "()\n"
       ]
      }
     ],
     "prompt_number": 31
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "feature_names = np.asarray(vectorizer.get_feature_names())\n",
      "feature_names"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 35,
       "text": [
        "array([u'00', u'000', u'0000', ..., u'zzzzzzt', u'\\xb3ation', u'\\xfd\\xe9'], \n",
        "      dtype='<U81')"
       ]
      }
     ],
     "prompt_number": 35
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#labels\n",
      "y_train"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 39,
       "text": [
        "array([7, 4, 4, ..., 3, 1, 8])"
       ]
      }
     ],
     "prompt_number": 39
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# note:  sample sizes are actually quite small for each category\n",
      "# probably not enough for true MC test\n",
      "for i in range(20):\n",
      "    print i, np.where(y_train==i)[0].size"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0 480\n",
        "1 584\n",
        "2 591\n",
        "3 590\n",
        "4 578\n",
        "5 593\n",
        "6 585\n",
        "7 594\n",
        "8 598\n",
        "9 597\n",
        "10 600\n",
        "11 595\n",
        "12 591\n",
        "13 594\n",
        "14 593\n",
        "15 599\n",
        "16 546\n",
        "17 564\n",
        "18 465\n",
        "19 377\n"
       ]
      }
     ],
     "prompt_number": 46
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# train MC classifier as baseline\n",
      "\n",
      "# build  1-vs-all data sets for all 20 categories\n",
      "#  start with 1 vs [2-20]\n",
      "#  \n",
      "# train 1-vs-all baseline and report accuracies\n",
      "# repeat with L+U and report accuracies for different size L's\n",
      "#   U and HO accuracies\n",
      "# \n",
      "# this is the baseline for further work\n",
      "#\n",
      "\n",
      "# generate svmlin inputs\n",
      "# run and test\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}